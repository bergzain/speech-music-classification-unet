{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:39:05.592211Z",
     "end_time": "2023-04-05T23:39:09.016576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# audio_file_path = \"/Users/zainhazzouri/projects/Bachelor_Thesis/Data1/Kaggle/music_wav/bartok.wav\"\n",
    "# audio_file_path = \"/Users/zainhazzouri/Desktop/egp1.mp3\"\n",
    "audio_file_path = \"/Users/zainhazzouri/projects/Bachelor_Thesis/Data1/Kaggle/music_wav/bagpipe.wav\"\n",
    "\n",
    "SAMPLE_RATE = 22050 # sample rate of the audio file\n",
    "bit_depth = 16 # bit depth of the audio file\n",
    "hop_length = 512\n",
    "n_mfcc = 20 # number of MFCCs features\n",
    "n_fft=1024, # window size\n",
    "n_mels = 256 # number of mel bands to generate\n",
    "win_length = None # window length\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:01.116244Z",
     "end_time": "2023-04-05T23:52:01.119823Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_built():  # if you have apple silicon mac\n",
    "    device = \"mps\"  # if it doesn't work try device = torch.device('mps')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:01.616515Z",
     "end_time": "2023-04-05T23:52:01.619863Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def preprocess(waveform, target_length=8000, sample_rate=SAMPLE_RATE, n_mfcc=n_mfcc):\n",
    "    waveform_length = waveform.size(1)\n",
    "\n",
    "    if waveform_length < target_length:\n",
    "        num_padding = target_length - waveform_length\n",
    "        padding = torch.zeros(1, num_padding)\n",
    "        waveform = torch.cat((waveform, padding), 1)\n",
    "    elif waveform_length > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "\n",
    "    mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc)(waveform)\n",
    "    return mfcc\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:01.827341Z",
     "end_time": "2023-04-05T23:52:01.831211Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WaveUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, num_features=40):\n",
    "        super(WaveUNet, self).__init__()\n",
    "\n",
    "        # Encoding layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, num_features, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_features, num_features * 2, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_features * 2, num_features * 4, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoding layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_features * 4, num_features * 2, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_features * 2, num_features, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_features, num_features, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:02.019940Z",
     "end_time": "2023-04-05T23:52:02.022570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "WaveUNet(\n  (encoder): Sequential(\n    (0): Conv2d(1, 40, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): ReLU()\n    (2): Conv2d(40, 80, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(80, 160, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (5): ReLU()\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(160, 80, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (1): ReLU()\n    (2): ConvTranspose2d(80, 40, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (3): ReLU()\n    (4): ConvTranspose2d(40, 40, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (5): ReLU()\n  )\n  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n  (fc): Linear(in_features=40, out_features=10, bias=True)\n)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WaveUNet().to(device)\n",
    "model.load_state_dict(torch.load(\"waveunet_speech_music_discrimination.pth\"))\n",
    "model.eval()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:02.184680Z",
     "end_time": "2023-04-05T23:52:02.205458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def split_waveform(waveform, sample_rate):\n",
    "    segment_length = sample_rate\n",
    "    num_segments = waveform.shape[-1] // segment_length\n",
    "    segments = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * segment_length\n",
    "        end = start + segment_length\n",
    "        segments.append(waveform[:, start:end])\n",
    "\n",
    "    return segments\n",
    "\n",
    "# TODO maybe only use Librosa for all types of files ,, this functions is causing problems\n",
    "def classify_audio_file_segments(audio_file_path):\n",
    "    file_ext = os.path.splitext(audio_file_path)[1].lower()\n",
    "\n",
    "    if file_ext == '.mp3':\n",
    "        waveform, sample_rate = librosa.load(audio_file_path, sr=SAMPLE_RATE)\n",
    "        waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "    else:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "\n",
    "    segments = split_waveform(waveform, sample_rate)\n",
    "\n",
    "    segment_classifications = []\n",
    "\n",
    "    for segment in segments:\n",
    "        mfcc = preprocess(segment, target_length=sample_rate)\n",
    "        mfcc = mfcc.to(device).unsqueeze(0)\n",
    "        output = model(mfcc)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        segment_classifications.append(predicted_class.item())\n",
    "\n",
    "    return segment_classifications\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:02.579438Z",
     "end_time": "2023-04-05T23:52:02.581910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time | End Time | Class\n",
      "----------------------------\n",
      "00:00:00 | 00:00:01 | 1\n",
      "00:00:01 | 00:00:30 | 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def format_time(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "\n",
    "def generate_classification_table(classification_results, segment_duration=1):\n",
    "    table = []\n",
    "    current_label = classification_results[0]\n",
    "    start_time = 0\n",
    "\n",
    "    for i, label in enumerate(classification_results[1:], 1):\n",
    "        if label != current_label:\n",
    "            table.append([format_time(start_time), format_time(i * segment_duration), current_label])\n",
    "            start_time = i * segment_duration\n",
    "            current_label = label\n",
    "\n",
    "    table.append([format_time(start_time), format_time(len(classification_results) * segment_duration), current_label])\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def save_classification_table_to_csv(table, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Start Time', 'End Time', 'Class'])\n",
    "        for row in table:\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "# print(audio_file_path)\n",
    "classification_results = classify_audio_file_segments(audio_file_path)\n",
    "table = generate_classification_table(classification_results)\n",
    "\n",
    "# Save the table to a CSV file\n",
    "save_classification_table_to_csv(table, \"classification_table.csv\")\n",
    "\n",
    "# Print the table\n",
    "print(\"Start Time | End Time | Class\")\n",
    "print(\"-\" * 28)\n",
    "for row in table:\n",
    "    print(f\"{row[0]} | {row[1]} | {row[2]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:03.001247Z",
     "end_time": "2023-04-05T23:52:03.204693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['00:00:00', '00:00:01', 1], ['00:00:01', '00:00:30', 0]]\n"
     ]
    }
   ],
   "source": [
    "classifications = classify_audio_file_segments(audio_file_path)\n",
    "table = generate_classification_table(classifications)\n",
    "print(table)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T23:52:32.139228Z",
     "end_time": "2023-04-05T23:52:32.299133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
