{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AttentionResidualWaveUNet' from 'CNN_Model' (CNN_Model.ipynb)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# importing the module from CNN_Model.ipynb\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnbimporter\u001B[39;00m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mCNN_Model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AttentionResidualWaveUNet\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'AttentionResidualWaveUNet' from 'CNN_Model' (CNN_Model.ipynb)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "# importing the module from CNN_Model.ipynb\n",
    "import nbimporter #\n",
    "from CNN_Model import AttentionResidualWaveUNet\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:25.945710Z",
     "end_time": "2023-04-15T14:50:30.429162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# audio_file_path = \"/Users/zainhazzouri/projects/Bachelor_Thesis/Data/Kaggle/music_wav/bartok.wav\"\n",
    "# audio_file_path = \"/Users/zainhazzouri/Desktop/egp1.mp3\"\n",
    "audio_file_path = \"/Users/zainhazzouri/projects/Bachelor_Thesis/Data/Kaggle/music_wav/bagpipe.wav\"\n",
    "\n",
    "SAMPLE_RATE = 22050 # sample rate of the audio file\n",
    "bit_depth = 16 # bit depth of the audio file\n",
    "hop_length = 512\n",
    "n_mfcc = 20 # number of MFCCs features\n",
    "n_fft=1024, # window size\n",
    "n_mels = 256 # number of mel bands to generate\n",
    "win_length = None # window length\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T15:00:45.461154Z",
     "end_time": "2023-04-15T15:00:45.464968Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_built():  # if you have apple silicon mac\n",
    "    device = \"mps\"  # if it doesn't work try device = torch.device('mps')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:30.432599Z",
     "end_time": "2023-04-15T14:50:30.434484Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess(waveform, target_length=8000, sample_rate=SAMPLE_RATE, n_mfcc=n_mfcc):\n",
    "    waveform_length = waveform.size(1)\n",
    "\n",
    "    if waveform_length < target_length:\n",
    "        num_padding = target_length - waveform_length\n",
    "        padding = torch.zeros(1, num_padding)\n",
    "        waveform = torch.cat((waveform, padding), 1)\n",
    "    elif waveform_length > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "\n",
    "    mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc)(waveform)\n",
    "    return mfcc\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:30.436577Z",
     "end_time": "2023-04-15T14:50:30.438011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "WaveUNet(\n  (encoder): Sequential(\n    (0): Conv2d(1, 40, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (1): ReLU()\n    (2): Conv2d(40, 80, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (3): ReLU()\n    (4): Conv2d(80, 160, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (5): ReLU()\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(160, 80, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (1): ReLU()\n    (2): ConvTranspose2d(80, 40, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (3): ReLU()\n    (4): ConvTranspose2d(40, 40, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n    (5): ReLU()\n  )\n  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n  (fc): Linear(in_features=40, out_features=10, bias=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttentionResidualWaveUNet().to(device)\n",
    "model.load_state_dict(torch.load(\"waveunet_speech_music_discrimination.pth\"))\n",
    "model.eval()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:30.440637Z",
     "end_time": "2023-04-15T14:50:30.501688Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def split_waveform(waveform, sample_rate):\n",
    "    segment_length = sample_rate\n",
    "    num_segments = waveform.shape[-1] // segment_length\n",
    "    segments = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = i * segment_length\n",
    "        end = start + segment_length\n",
    "        segments.append(waveform[:, start:end])\n",
    "\n",
    "    return segments\n",
    "\n",
    "# TODO maybe only use Librosa for all types of files ,, this functions is causing problems\n",
    "def classify_audio_file_segments(audio_file_path):\n",
    "    file_ext = os.path.splitext(audio_file_path)[1].lower()\n",
    "\n",
    "    if file_ext == '.mp3':\n",
    "        waveform, sample_rate = librosa.load(audio_file_path, sr=SAMPLE_RATE)\n",
    "        waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "    else:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file_path)\n",
    "\n",
    "    segments = split_waveform(waveform, sample_rate)\n",
    "\n",
    "    segment_classifications = []\n",
    "\n",
    "    for segment in segments:\n",
    "        mfcc = preprocess(segment, target_length=sample_rate)\n",
    "        mfcc = mfcc.to(device).unsqueeze(0)\n",
    "        output = model(mfcc)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        segment_classifications.append(predicted_class.item())\n",
    "\n",
    "    return segment_classifications\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:30.501362Z",
     "end_time": "2023-04-15T14:50:30.503646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zainhazzouri/miniforge3/envs/Bachelor_Thesis/lib/python3.8/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time | End Time | Class\n",
      "----------------------------\n",
      "00:00:00 | 00:00:30 | 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def format_time(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "\n",
    "def generate_classification_table(classification_results, segment_duration=1):\n",
    "    table = []\n",
    "    current_label = classification_results[0]\n",
    "    start_time = 0\n",
    "\n",
    "    for i, label in enumerate(classification_results[1:], 1):\n",
    "        if label != current_label:\n",
    "            table.append([format_time(start_time), format_time(i * segment_duration), current_label])\n",
    "            start_time = i * segment_duration\n",
    "            current_label = label\n",
    "\n",
    "    table.append([format_time(start_time), format_time(len(classification_results) * segment_duration), current_label])\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def save_classification_table_to_csv(table, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Start Time', 'End Time', 'Class'])\n",
    "        for row in table:\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "# print(audio_file_path)\n",
    "classification_results = classify_audio_file_segments(audio_file_path)\n",
    "table = generate_classification_table(classification_results)\n",
    "\n",
    "# Save the table to a CSV file\n",
    "save_classification_table_to_csv(table, \"classification_table.csv\")\n",
    "\n",
    "# Print the table\n",
    "print(\"Start Time | End Time | Class\")\n",
    "print(\"-\" * 28)\n",
    "for row in table:\n",
    "    print(f\"{row[0]} | {row[1]} | {row[2]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:30.505881Z",
     "end_time": "2023-04-15T14:50:30.890689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['00:00:00', '00:00:30', 0]]\n"
     ]
    }
   ],
   "source": [
    "classifications = classify_audio_file_segments(audio_file_path)\n",
    "table = generate_classification_table(classifications)\n",
    "print(table)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T14:50:30.892407Z",
     "end_time": "2023-04-15T14:50:31.082535Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
