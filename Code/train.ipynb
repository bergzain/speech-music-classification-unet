{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"/Users/zainhazzouri/projects/Bachelor_Thesis/Data/Kaggle\"\n",
    "SAMPLE_RATE = 22050 # sample rate of the audio file\n",
    "bit_depth = 16 # bit depth of the audio file\n",
    "hop_length = 512\n",
    "n_mfcc = 20 # number of MFCCs features\n",
    "n_fft=1024, # window size\n",
    "n_mels = 256 # number of mel bands to generate\n",
    "win_length = None # window length\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:51:31.099434Z",
     "end_time": "2023-04-14T16:51:31.103018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:45:52.487945Z",
     "end_time": "2023-04-14T16:45:52.493246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nbimporter\n",
    "from CNN_Model import WaveUNet\n",
    "\n",
    "class SpeechMusicDataset(Dataset):\n",
    "    def __init__(self, music_waves, speech_waves, transform=None):\n",
    "        self.music_waves = music_waves\n",
    "        self.speech_waves = speech_waves\n",
    "        self.transform = transform\n",
    "        self.file_list = self.music_waves + self.speech_waves\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        waveform, _ = torchaudio.load(file_path)\n",
    "        label = 0 if 'music_wav' in file_path else 1\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, label\n",
    "\n",
    "# Preprocessing function for the dataset\n",
    "def preprocess(waveform, target_length=8000, sample_rate=SAMPLE_RATE, n_mfcc=n_mfcc):\n",
    "    waveform_length = waveform.size(1)\n",
    "\n",
    "    if waveform_length < target_length:\n",
    "        num_padding = target_length - waveform_length\n",
    "        padding = torch.zeros(1, num_padding)\n",
    "        waveform = torch.cat((waveform, padding), 1)\n",
    "    elif waveform_length > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "\n",
    "    mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc)(waveform)\n",
    "    return mfcc\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_built():  # if you have apple silicon mac\n",
    "    device = \"mps\"  # if it doesn't work try device = torch.device('mps')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Set the path to the folder containing the music and speech datasets\n",
    "AUDIO_DIR = \"/Users/zainhazzouri/projects/Bachelor_Thesis/Data/Kaggle/\"\n",
    "\n",
    "# Load the dataset\n",
    "music_waves = glob.glob(AUDIO_DIR + \"music_wav\" + \"/*.wav\")\n",
    "speech_waves = glob.glob(AUDIO_DIR + \"speech_wav\" + \"/*.wav\")\n",
    "transform = preprocess\n",
    "\n",
    "dataset = SpeechMusicDataset(music_waves, speech_waves, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_waveform(waveform, desired_length):\n",
    "    if waveform.shape[-1] < desired_length:\n",
    "        padding = desired_length - waveform.shape[-1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "    return waveform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "music_waves = glob.glob(AUDIO_DIR + \"music_wav\" + \"/*.wav\")\n",
    "speech_waves = glob.glob(AUDIO_DIR + \"speech_wav\" + \"/*.wav\")\n",
    "transform = preprocess\n",
    "\n",
    "dataset = SpeechMusicDataset(music_waves, speech_waves, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = WaveUNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:51:32.982781Z",
     "end_time": "2023-04-14T16:51:33.000375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9286\n",
      "Epoch: 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2733\n",
      "Epoch: 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8217\n",
      "Epoch: 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8082\n",
      "Epoch: 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6834\n",
      "Epoch: 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8173\n",
      "Epoch: 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7277\n",
      "Epoch: 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7046\n",
      "Epoch: 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7017\n",
      "Epoch: 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7015\n",
      "Epoch: 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7053\n",
      "Epoch: 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7337\n",
      "Epoch: 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7153\n",
      "Epoch: 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6844\n",
      "Epoch: 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6701\n",
      "Epoch: 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7081\n",
      "Epoch: 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6846\n",
      "Epoch: 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6680\n",
      "Epoch: 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6737\n",
      "Epoch: 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6415\n",
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Add tqdm progress bar\n",
    "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=\"Training\", ncols=100)):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:51:33.788781Z",
     "end_time": "2023-04-14T16:51:43.344107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7501\n",
      "Validation Accuracy: 38.46%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "\n",
    "#1. Average validation loss: This metric is calculated using the same loss function (`criterion`) used during training, which is CrossEntropyLoss in this case. The average validation loss is computed by summing the losses for all validation samples and then dividing by the number of validation samples. A lower average validation loss indicates better performance.\n",
    "#\n",
    "# 2. Validation accuracy: This metric measures the percentage of correctly classified samples in the validation set. The accuracy is calculated by counting the number of correct predictions, i.e., when the predicted label matches the true label, and then dividing by the total number of validation samples. A higher validation accuracy indicates better performance.\n",
    "#\n",
    "# These two metrics together provide a good evaluation of the model's performance on the validation set. The average validation loss helps assess the model's ability to minimize the loss function, while the validation accuracy measures how well the model is classifying the samples.\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = evaluate(val_loader, model, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:51:56.611781Z",
     "end_time": "2023-04-14T16:51:56.749783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"waveunet_speech_music_discrimination.pth\")\n",
    "print(\"Model saved.\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:50:18.240287Z",
     "end_time": "2023-04-14T16:50:18.285291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 40, 20, 216]           1,040\n",
      "              ReLU-2          [-1, 40, 20, 216]               0\n",
      "            Conv2d-3          [-1, 80, 10, 108]          80,080\n",
      "              ReLU-4          [-1, 80, 10, 108]               0\n",
      "            Conv2d-5           [-1, 160, 5, 54]         320,160\n",
      "              ReLU-6           [-1, 160, 5, 54]               0\n",
      "   ConvTranspose2d-7          [-1, 80, 10, 108]         320,080\n",
      "              ReLU-8          [-1, 80, 10, 108]               0\n",
      "   ConvTranspose2d-9          [-1, 40, 20, 216]          80,040\n",
      "             ReLU-10          [-1, 40, 20, 216]               0\n",
      "  ConvTranspose2d-11          [-1, 40, 40, 432]          40,040\n",
      "             ReLU-12          [-1, 40, 40, 432]               0\n",
      "AdaptiveAvgPool2d-13             [-1, 40, 1, 1]               0\n",
      "           Linear-14                   [-1, 10]             410\n",
      "================================================================\n",
      "Total params: 841,850\n",
      "Trainable params: 841,850\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.07\n",
      "Forward/backward pass size (MB): 19.12\n",
      "Params size (MB): 3.21\n",
      "Estimated Total Size (MB): 22.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show model summary\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "    print(\"\\nModel summary:\")\n",
    "\n",
    "    original_device = device\n",
    "    if device == 'mps': # because MPS is not supported by torchsummary\n",
    "        device = 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    summary(model, input_size=(1, 40, 431), device=device)\n",
    "\n",
    "    if original_device == 'mps': # Restore original device\n",
    "        device = original_device\n",
    "        model.to(device)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nPlease install torchsummary to display the model summary. Use `pip install torchsummary`.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:50:23.103927Z",
     "end_time": "2023-04-14T16:50:23.161762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
